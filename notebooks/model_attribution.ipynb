{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c597f4f-0c21-4b34-b05a-5130d5421b84",
   "metadata": {},
   "source": [
    "# Multi-model attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e93227-4f85-45a2-83cc-516ecef3ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import attribution.funcs\n",
    "import attribution.validation\n",
    "import attribution.preprocessing\n",
    "import attribution.bootstrap\n",
    "import iris\n",
    "import iris_utils\n",
    "from climix.metadata import load_metadata\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from functools import partial\n",
    "import geopandas as gpd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.stats as scstats\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883a680-9bca-412a-bcca-3f95a0b5dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust this address from gridClim notebook.\n",
    "# client = Client(\"127.0.0.1:38409\")\n",
    "client = Client(processes=True, threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344728c8-89e9-4309-90b2-29a7f272c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/nobackup/rossby26/users/sm_erhol/extremeEventAttribution/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c275a4c6-c2a6-4f03-959d-e1fc99a4292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains shapes of most countries in the world.\n",
    "# https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-0-boundary-lines/\n",
    "fname = \"/home/sm_erhol/data/ne_10_admin_0_countries/ne_10m_admin_0_countries.shp\"\n",
    "\n",
    "gdf = gpd.read_file(fname)\n",
    "\n",
    "# Select Sweden.\n",
    "swe_shapes = gdf[gdf.SOVEREIGNT == \"Sweden\"].geometry\n",
    "swe_mainland = swe_shapes.iloc[0].geoms[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859af6d-f382-442b-a233-938b9c8899ef",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1662f13-dc4e-4bec-8935-f042178b249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cordex_cube = iris.load_cube(\n",
    "    os.path.join(data_path, \"prAdjust_Gavle_CORDEX-ENS_rcp85_day_19710101-20181230.nc\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5af7a2-c755-464c-93a5-e20f4205e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cordex_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31346a75-70e9-4899-997c-4e0cfe9a4e1a",
   "metadata": {},
   "source": [
    "## Event definition\n",
    "\n",
    "161 mm in 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a4eb0-1106-4f65-bda5-b2faa947b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 161"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480b084f-6cbf-4441-bfe8-09fd6b445be0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Rx1\n",
    "Use climix to compute Rx1 for the cube."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157745e6-e04b-45fe-8a84-a9fa71881bbb",
   "metadata": {},
   "source": [
    "Get the annual maximums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f842537c-d235-4ae7-a52a-674fbd04e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index catalog\n",
    "catalog = load_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad1a5d-f4b6-4aa8-a21f-a9ae7ee6f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx1_ann_index = catalog.prepare_indices([\"rx1day\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500abf9-aa41-4a05-b0de-8fe7f21c3a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't have a year coordiante when passing to climix.\n",
    "try:\n",
    "    cordex_cube.remove_coord(\"year\")\n",
    "except iris.exceptions.CoordinateNotFoundError:\n",
    "    pass\n",
    "rx1_ann = rx1_ann_index([cordex_cube], client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7f11d-d408-40e2-8397-182b18853d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask.\n",
    "mask = iris_utils.mask_from_shape(\n",
    "    rx1_ann[0, :, :, :], swe_mainland, coord_names=(\"grid_latitude\", \"grid_longitude\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64df17e-9e51-4561-b747-991ae97693b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.broadcast_to(mask, rx1_ann.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5962e-734c-4e51-ae5a-527b309842ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This mask inplace as well.\n",
    "iris_utils.mask_cube(rx1_ann, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd0fae-3531-4c67-937b-a1cbb77837a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, density is way above one since the bin values are so small.\n",
    "# e.g. the widht of each bin is ~0.0001, hence integrating = 1\n",
    "plt.hist(rx1_ann[2, :, :, :].data.compressed(), density=True)\n",
    "plt.xlabel(\"Rx1 annual\")\n",
    "plt.ylabel(\"Density\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ab60b-fec6-49d0-a755-86f934478616",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Validate dist params of models\n",
    "We should check how the models are representing the extreme values.\n",
    "How this should be done is an open question.\n",
    "Comparing the CI of the distribution fit parameters is easier than it sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf91e0f-5b04-445e-9eb3-9ef6b1d7aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_fits = np.load(os.path.join(data_path, \"etc/rx1-ann_fits_ci_gridclim.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207d593-a9a8-44bc-860d-6b29a881a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_fits.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11cc9ce-d642-4280-9935-7d6617fbe7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6c448-136e-4061-848d-dff1c2ab6907",
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = rng.integers(0, 100, (2, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf0f84d-9310-433c-8ecf-a1e69e71ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rng.normal(size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507640ce-c1fb-464a-981f-7126741d3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3ae95-d860-4120-9252-b45ba3fdaec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[..., inds[0, :]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91687272-b05e-4c7a-990f-3bf1b819fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[..., inds[1, :]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d7b28-cda0-4fde-950f-65af0d129593",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_fits.min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd864c98-4024-425e-9011-8bc98c83e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc_fits_ci = np.quantile(gc_fits, [0.05, 0.5, 0.95], axis=0, method=\"median_unbiased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb240f-7d3c-4cbe-a975-54d9ad238dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_fits - gc_fits.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0abf15-61c4-4173-9da3-c5bb33ee068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_error = np.std(gc_fits, axis=0) / np.sqrt(gc_fits.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9fb79-60d4-49da-afb5-978d79b8f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_fits.mean(axis=0) + std_error * 1.645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b2b65-46be-4fb2-b30e-de4cfb6d8a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_fits.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c6cc3-7fc4-4a57-ac25-064b8211b1d8",
   "metadata": {},
   "source": [
    "#### Fit CORDEX data\n",
    "We need to compress each ensemble member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c7c40-9d7b-47d6-9c62-c3415fbf0aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the GEV dist object\n",
    "dist = scstats.genextreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec5bd6-ce66-4e42-878c-c148525cf81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the data of the rx1 cube.\n",
    "rx1_ann_data = np.zeros(\n",
    "    (rx1_ann.shape[0], rx1_ann.data[0, :, :, :].compressed().shape[0])\n",
    ")\n",
    "# We need to compress the data for each year. This has to be done\n",
    "# in a loop I think.\n",
    "for i, year in enumerate(rx1_ann.data):\n",
    "    rx1_ann_data[i] = year.compressed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2607dde-15f2-442a-aa77-dc347fa25559",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx1_ann_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e29f06b-0f77-4364-98ea-7bb603aba558",
   "metadata": {},
   "source": [
    "For Cordex, we don't need to run the bootstrap, since the CI is \"provided\" by the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d2a2eb-5141-4e98-87cf-edda407e8b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small function to distribute.\n",
    "def fit_dist(data):\n",
    "    return dist.fit(data.compressed())\n",
    "\n",
    "\n",
    "# Do the fits in paralell.\n",
    "results = client.map(fit_dist, rx1_ann_data)\n",
    "# Gather the results.\n",
    "results = client.gather(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da6555c-03ac-49a1-8128-2e82d16c903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want the results as an array.\n",
    "results = np.asarray(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfecb6e7-c29f-4e83-9e63-9ceff06674d3",
   "metadata": {},
   "source": [
    "We can check which of the ensemble distributions that has parameters that lie within the CI of GridClim distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49b80b-2aa4-45e5-b387-095f6d51f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_fits_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13975bc-98e3-41a6-9873-0e3132d22dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We allow for a 5% buffer of the CI.\n",
    "ok_dists = attribution.validation.check_dist_params(results, gc_fits_ci, buffer=0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfeb976-790b-42ea-97ef-23b53b184f91",
   "metadata": {},
   "source": [
    "~~For now we don't do anything with these results, since none of the members pass the check.\n",
    "This seems a bit odd, and is likely due to a very narrow CI of GridClim.\n",
    "Until the CI of GridClim is final we don't exclude because of this.~~\n",
    "\n",
    "We only use the ensemble members that passed the distribution check.\n",
    "The use of a buffer is questionable, but this used to allow some leeway to the very narrow CI of GridClim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486704f-6d35-4869-8eb3-03cc2f50d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad01c59-b1de-4446-8e0c-bc6db24e7113",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_fits_ci[2, :] * [0.5, 0.05, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7e1d07-089c-4a02-b084-ffbb774aa220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the CI of the dist parameters.\n",
    "# We only use the ensemble members that passed the distribution check.\n",
    "fits_ci = np.quantile(results, [0.05, 0.5, 0.95], axis=0, method=\"median_unbiased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1733171-1461-4a48-bf96-2ab3ef6bcba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fits_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e424c-e52b-4444-b305-fb0718063777",
   "metadata": {},
   "source": [
    "### Regression to GMST\n",
    "To scale the distribution with the use of GMST we first need to fit a regression between the Rx1 and GMST.\n",
    "The slope of the regression can then be used for the scaling.\n",
    "\n",
    "First we load the GISTEMP data from NASA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee753a6-3a7d-4e6c-a96d-8d93b15909eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to gmst\n",
    "gmst_path = os.path.join(data_path, \"etc/gistemp.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac71b544-4e82-4d3c-90eb-359c42bc583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the smoothed gmst data  for the timespan\n",
    "# covered by the cube.\n",
    "gmst_data = attribution.funcs.get_gmst(rx1_ann[0, :, :, :], path=gmst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30157101-7e65-4309-9828-e499011da33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the data of the rx1 cube.\n",
    "shape = rx1_ann.shape\n",
    "# What is the shape of one compressed year.\n",
    "comp_shape = rx1_ann[0, 0, :, :].data.compressed().shape\n",
    "# The final shape.\n",
    "shape = (shape[0], shape[1], comp_shape[0])\n",
    "rx1_ann_data = np.zeros(shape)\n",
    "# We need to compress the data for each year to properly remove the masked data.\n",
    "for i, member in enumerate(rx1_ann.data):\n",
    "    for j, year in enumerate(member):\n",
    "        rx1_ann_data[i, j] = year.compressed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57aaa63-781c-4c32-baae-4cf2f90a3752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that dimension of the year match.\n",
    "assert rx1_ann_data.shape[1] == gmst_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8f4ee-9695-45cd-9a44-0b1a3d00d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can make clever use of the multiregression feature, since we want\n",
    "# know the regression for each point.\n",
    "reg_partial = partial(LinearRegression().fit, gmst_data)\n",
    "reg = client.map(reg_partial, rx1_ann_data)\n",
    "reg = client.gather(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43359408-a14f-4c6e-aad4-0182ccccf195",
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes = [reg.coef_.flatten() for reg in reg]\n",
    "\n",
    "# We like arrays.\n",
    "slopes = np.asarray(slopes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff781d8-2ab8-47d9-aeba-59c522e82fbf",
   "metadata": {},
   "source": [
    "### Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d04161-9e1c-4dbe-b42c-e9646a463c46",
   "metadata": {},
   "source": [
    "Here we compute the probability ratios for the event.\n",
    "First we create the partial function which we can distribute on the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc911e36-9d4b-4ad7-95e0-0210d812e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_prob_ratio_p = partial(\n",
    "    attribution.funcs.calc_prob_ratio, threshold=threshold, temperature=-1.2, dist=dist\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5710d439-7688-48b0-b878-86108b8de521",
   "metadata": {},
   "source": [
    "Reshape the data to pool it for each ensemble member.\n",
    "We have already compressed it for each ensemble member and year above, which removed the masked data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9cdd20-dba2-4ec6-953c-4d9031e55490",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx1_ann_data = rx1_ann_data.reshape(shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea491dcf-17b3-4922-9387-0241b637f8ce",
   "metadata": {},
   "source": [
    "Then we want to resample the ensemble members and randomly select which of their respective regression slopes to use for the computation of the probability ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1f620-8e6e-4f18-8dc9-6846293a919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need a random number generator.\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f91a42c-669e-41fd-988f-1501cfef5cb7",
   "metadata": {},
   "source": [
    "First, create resamples for the ensemble member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cded6dd-940f-4bc7-8ca2-1d570fe123f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generates n_resamples between 0 and 65\n",
    "n_resamples = 1000\n",
    "resamples = rng.integers(shape[0], size=n_resamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5129743-8076-4817-a941-3961be2d55c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then select both slopes and the rx1 data\n",
    "sampled_slopes = slopes[resamples, :]\n",
    "sampled_data = rx1_ann_data[resamples, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3412fa-efcd-430b-ac35-39ee411d8313",
   "metadata": {},
   "source": [
    "For the slopes we have to do a second resampling, to get one slope for each ensemble member.\n",
    "Since `sampled_slopes` is a view of `slopes` we have to generate random indices and use `take_along_axis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f55b0-283c-4e86-a762-f9218a78f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n_resmaples integers between 0 and 508 (exclusive).\n",
    "indices = rng.integers(shape[2], size=(n_resamples, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6473e069-c76c-4b3f-b0ab-d02c056434b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rng.choice does not work here, since it returns the same values when asked to do \n",
    "# a choice from views of the same array.\n",
    "sampled_slopes = np.take_along_axis(sampled_slopes, indices, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1605f7-2a74-4b76-aca8-d050f6821c61",
   "metadata": {},
   "source": [
    "Now we can distribute the tasks of calculating the probability ratio for each ensemble member - slope pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271eee9-5c2d-4130-806f-eab2ba1b8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the tasks to the client.\n",
    "prob_ratios = client.map(\n",
    "    calc_prob_ratio_p, sampled_data, sampled_slopes\n",
    ")\n",
    "# And collect it.\n",
    "prob_ratios = client.gather(prob_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a2652-bd86-4a62-acbc-12ec59048c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We like arrays.\n",
    "prob_ratios = np.asarray(prob_ratios)\n",
    "# Remove infinite values.\n",
    "prob_ratios = prob_ratios[~np.isinf(prob_ratios)]\n",
    "# Maybe also remove unrealistically large values?\n",
    "prob_ratios = prob_ratios[prob_ratios < 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd58fe6-de90-4a0f-b34e-0b9bfe87e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are going to be some very large values here.\n",
    "plt.hist(prob_ratios);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb5ba6-f650-4211-8f99-765bdba00e5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Bca\n",
    "A question here is we should use a normal percentile interval or if we should try and calculate the Bca interval.\n",
    "It will be a bit complicated.\n",
    "How would we do the jackknife here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311bc7a9-a18f-4eaf-96d7-932934d69af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "jackknife_resample = attribution.bootstrap.jackknife_resample(np.zeros(n_resamples), batch=1)\n",
    "jackknife_resample = np.asarray(list(jackknife_resample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e164c5e-73fc-4828-b165-81e709e2646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jackknife_resample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b81f0e-af7f-4955-b6d2-1f5104c02609",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_jackknife = sampled_data[..., jackknife_resample[:, :, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a8fcf-f191-4999-9220-8a11809ce2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_jackknife = sampled_slopes[jackknife_resample[:, :, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e228a-1bdc-4844-bae7-8d693699a397",
   "metadata": {},
   "source": [
    "#### Calculate CI\n",
    "Until we figure out how to do the Bca on this, we simply do a percentile CI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c41bc-b891-4735-9b90-5ae366bb79c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_ratios_ci = np.percentile(prob_ratios, [5, 50, 95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e229e50-9d5f-4e6a-8896-4fd4c3fd1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_ratios_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd479847-fd1d-483a-963a-1b6b135d7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(data_path, \"etc/rx1-ann_prb_cordex\"), prob_ratios_ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b184d-b1c4-4a4e-aa70-0494b053dac4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Rx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe5a05-c2f3-40cf-977f-19016ee3e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a year categorisation\n",
    "iris.coord_categorisation.add_year(cordex_cube, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d340da6-2515-4ccf-89aa-d36129f75bf6",
   "metadata": {},
   "source": [
    "Get the annual maximums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4423a-e68f-4cfd-9e76-416bf35f55f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx1_ann = cordex_cube.aggregated_by(\"year\", iris.analysis.MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e56812-ad14-47d1-b3b1-94ceb6b08e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, density is way above one since the bin values are so small.\n",
    "# e.g. the widht of each bin is ~0.0001, hence integrating = 1\n",
    "plt.hist(rx1_ann[2, :, :, :].data.compressed(), density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a88445d-7e0f-4cc9-a7b3-ef2efb3de72a",
   "metadata": {},
   "source": [
    "### Fit a GEV distribution to Rx2.\n",
    "We use scipy to fit a GEV distribution to this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca95b5-d405-4930-a1d0-b841b34bafea",
   "metadata": {},
   "source": [
    "### Load dist params CI for GridClim\n",
    "We should check how the models are representing the extreme values as well.\n",
    "Initial checks show that the models have more, and higher, extremes compared to the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed99be2-3020-45d7-bfa2-1ba8cdfb8355",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_fits = np.load(os.path.join(data_path, \"etc/rx2-ann_fits_ci_gridclim.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e7703-0f23-4b38-b617-a9faed31387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_fits_ci = np.quantile(gc_fits, [0.05, 0.5, 0.95], axis=0, method=\"median_unbiased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556d04f-d764-4fc3-99cc-f7f2ec983a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_fits_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c04c74-15cd-4b28-b91e-8b80f9c9dcb2",
   "metadata": {},
   "source": [
    "### Fit CORDEX data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251a02f-dbee-44c6-92ed-384fce0ef8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the GEV dist object\n",
    "dist = scstats.genextreme\n",
    "# data\n",
    "data = rx1_ann.data.reshape(rx1_ann.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f267c5b2-5111-4bbb-b512-025af79e5660",
   "metadata": {},
   "source": [
    "For Cordex, we don't need to run the bootstrap, since the CI is \"provided\" by the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0041387a-cb18-4552-9782-003df0c09058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small function to distribute.\n",
    "def fit_dist(data):\n",
    "    return dist.fit(data.compressed())\n",
    "\n",
    "\n",
    "# Do the fits in paralell.\n",
    "results = client.map(fit_dist, data)\n",
    "# Gather the results.\n",
    "results = client.gather(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf3b6f-a8cd-4c3d-8d07-0fa001fe7f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want the results as an array.\n",
    "results = np.asarray(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5781e21b-bba0-4f14-b1be-c5b254b93def",
   "metadata": {},
   "source": [
    "We can check which of the ensemble distributions that has parameters that lie within the CI of GridClim distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb040b01-5afd-4bac-9246-08cbeb486cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We allow for a 5% buffer of the CI.\n",
    "ok_dists = attribution.validation.check_dist_params(results, gc_fits_ci, buffer=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c72ff-76dd-4280-9b93-2165224b755f",
   "metadata": {},
   "source": [
    "~~For now we don't do anything with these results, since none of the members pass the check.\n",
    "This seems a bit odd, and is likely due to a very narrow CI of GridClim.\n",
    "Until the CI of GridClim is final we don't exclude because of this.~~\n",
    "\n",
    "We only use the ensemble members that passed the distribution check.\n",
    "The use of a ==buffer== is questionable, but this used to allow some leeway to the very narrow CI of GridClim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f7429-349d-4aae-a6b2-4904a2be8007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the CI of the dist parameters.\n",
    "# We only use the ensemble members that passed the distribution check.\n",
    "fits_ci = np.quantile(\n",
    "    results[ok_dists], [0.05, 0.5, 0.95], axis=0, method=\"median_unbiased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c477914-25e2-4bd9-9f94-1c4ce3f1190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fits_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57459c2-f498-47a8-8f40-b011043313a6",
   "metadata": {},
   "source": [
    "### Regression to GMST\n",
    "To scale the above distribution with the use of GMST we first need to fit a regression between the Rx1 and GMST.\n",
    "The slope of the regression can then be used for the scaling.\n",
    "\n",
    "But first we load the GISTEMP data from NASA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec821fd-0c5d-442b-8c30-1a32aea96855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to gmst\n",
    "gmst_path = os.path.join(data_path, \"etc/gistemp.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d4c31d-f554-4162-a99a-e5a6497de411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the smoothed gmst data  for the timespan\n",
    "# covered by the cube.\n",
    "gmst_data = attribution.funcs.get_gmst(cordex_cube, path=gmst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657a33b-e493-4577-a68f-f3bfe0f3dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the data of the rx1 cube.\n",
    "# Reshape to flatten the spatial dimensions.\n",
    "rx1_ann_data = rx1_ann.data[ok_dists, :, :]\n",
    "rx1_ann_data = rx1_ann_data.reshape(rx1_ann_data.shape[0], rx1_ann.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f8636-7b57-4b66-847d-93f04c49f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx1_ann_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b45a77-af16-45af-88d3-39cbea21de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that first dimensions match.\n",
    "assert rx1_ann_data.shape[1] == gmst_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98485de6-de73-4d12-9407-7eb6330d3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can make clever use of the multiregression feature, we want\n",
    "# know the regression for each point.\n",
    "reg_partial = partial(LinearRegression().fit, gmst_data)\n",
    "reg = client.map(reg_partial, rx1_ann_data)\n",
    "reg = client.gather(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e63e2-9d96-41c5-94cc-1b7b7a53403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives a n x 800 list\n",
    "slopes = [reg.coef_.flatten() for reg in reg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5529752e-2b7d-4e63-b1b5-f759910fe8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We like arrays.\n",
    "slopes = np.asarray(slopes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f9fdd-fa28-4c28-af2c-7c36992a44c1",
   "metadata": {},
   "source": [
    "### Scale distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d613467-6dd3-4ab2-9ee8-7ba2eb6fbb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create current climate dists with CI\n",
    "dists_ci = [dist(*fit) for fit in fits_ci]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b9355a-5eaf-4d55-86d0-a0a31215fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we get all the scaled distributions.\n",
    "all_scaled_dists = attribution.funcs.scale_distributions(fits_ci, slopes, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc8ef9-9353-48c2-8e55-bbbc14ebef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This doesn't really tell us much TBH.\n",
    "attribution.plotting.plot_distribution(\n",
    "    data.compressed(), dists_ci, all_scaled_dists, title=\"Rx2 CORDEX\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d2d01-6d81-40d8-a63f-b6aa911a5ebf",
   "metadata": {},
   "source": [
    "### Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8abda26-536c-4d5b-84b5-a387ce850767",
   "metadata": {},
   "source": [
    "The probability ratio(s) (PR) for an event the magnitude of the Gävle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc5e382-1b88-4936-9a75-7a04e34ff495",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_ratios = attribution.funcs.get_probability_ratios(\n",
    "    dists_ci, all_scaled_dists, threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9297f2a0-3520-45b3-bc43-40d79c023a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(data_path, \"etc/rx2-ann_prb_cordex\"), prob_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b855a-ebaa-4955-9469-e9516f2c3a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2936e1-02bc-4d1d-8059-c832db0ec945",
   "metadata": {},
   "source": [
    "## Next step\n",
    "\n",
    "[Synthesis](./synthesis.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
