{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff2cf07-bc99-46ad-807f-af0629c1a4e3",
   "metadata": {},
   "source": [
    "# GridClim analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bcb45f-6ee2-4ea2-b1e1-98fb19b96b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small helper lib.\n",
    "import attribution.funcs\n",
    "import attribution.bootstrap\n",
    "\n",
    "# Others.\n",
    "import iris\n",
    "import iris.coord_categorisation\n",
    "import iris.plot as iplt\n",
    "import iris_utils\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as scstats\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from climix.metadata import load_metadata\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "import glob\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af0684-bae3-4af6-886c-b49de28a529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/nobackup/rossby26/users/sm_erhol/extremeEventAttribution/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70888b08-d86a-4563-9c48-072a1070a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(processes=True, threads_per_worker=1)\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ba22c-21fa-4d73-a2c6-7914a17c4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.scheduler_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bdf9bb-7a10-4d88-b86c-4dcfa9168d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sweref projection.\n",
    "sweref = ccrs.epsg(3006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7fb11-0f66-4f69-9ebd-e805875e5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains shapes of most countries in the world.\n",
    "# https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-0-boundary-lines/\n",
    "fname = \"/home/sm_erhol/data/ne_10_admin_0_countries/ne_10m_admin_0_countries.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96c871-9d63-4d73-bbd9-9fe21164abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ce2c5-8935-44d7-be5f-ebd551bf2fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Sweden.\n",
    "swe_shapes = gdf[gdf.SOVEREIGNT == \"Sweden\"].geometry\n",
    "swe_mainland = swe_shapes.iloc[0].geoms[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08eb95c-c36b-4317-b63a-546feef31dc5",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "Let's load the SweGridClim data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b3290-66c1-429f-8cdd-11557627feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/nobackup/smhid17/proj/sik/SMHIGridClim_NORDIC-11/v0.9/netcdf/day/pr/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a478118-f3be-4dcf-8a86-fd49c45acefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives a list of files in the base path matchig the wildcard.\n",
    "files = glob.glob(base_path + \"*.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9c60e-ed79-4fe2-a7c5-7740dd794d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = iris.load(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa9c6ba-93a0-407a-b1f1-5145e3e74ca1",
   "metadata": {},
   "source": [
    "We want to concatenate these cubes to one.\n",
    "But have to remove some attributes first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd035f1-7456-4380-aa78-20e96c897f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed = iris.util.equalise_attributes(cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78054793-4611-46ac-8552-881602bbbc4e",
   "metadata": {},
   "source": [
    "Now we should hopefully be able to concatenate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db19f03b-e3ce-4a2f-bdeb-38924637dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We concat on time.\n",
    "cube = cube.concatenate_cube()\n",
    "time_constraint = iris.Constraint(time=lambda cell: 1971 <= cell.point.year <= 2018)\n",
    "# Extract the relevant time period.\n",
    "cube = cube.extract(time_constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ae219-eea5-42e9-88f4-80c52ea975df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec34aec8-c567-42ca-9109-d3e43815926a",
   "metadata": {},
   "source": [
    "Extract data for Sweden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac7383-db1b-45fd-8239-1d468dba3ed9",
   "metadata": {},
   "source": [
    "## Region selection\n",
    "We probably don't want to look over all of Sweden.\n",
    "Which region should we select the data over?\n",
    "Some box around Gävle, where data should be homogeneous.\n",
    "\n",
    "Could make an average map and use this to select an area around POI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b2eee3-e49d-4277-a12a-1ea60926e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_cube = cube.collapsed(\"time\", iris.analysis.MEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8f4bc-6604-43ce-adfc-a3f5ac75af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_points = [[14.5, 14.5, 19.5, 19.5], [57.7, 61.2, 57.7, 61.2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0af37b-bd1c-43db-9c33-f5b89eaaf1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gävle point\n",
    "lat = 60.73284099330242\n",
    "lon = 17.09885344649177\n",
    "fig, ax = plt.subplots(figsize=(7, 9), subplot_kw={\"projection\": sweref})\n",
    "iplt.contourf(clim_cube, 30, axes=ax)\n",
    "ax.scatter([lon], [lat], s=50, transform=ccrs.PlateCarree(), label=\"Gävle\")\n",
    "ax.scatter(\n",
    "    mask_points[0],\n",
    "    mask_points[1],\n",
    "    s=50,\n",
    "    c=\"k\",\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    label=\"Box corners\",\n",
    ")\n",
    "\n",
    "ax.coastlines()\n",
    "ax.legend()\n",
    "ax.set_title(\"Average precipitation flux\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43abbc4-1cf6-4acf-8f37-19fbd8866989",
   "metadata": {},
   "source": [
    "We then have to convert the coordinates to the CoordSystem of our cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ca1ba-ef09-4a5c-93e2-23d5a3f3a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coord system of the cube. Convert it to cartopy.\n",
    "target_projection = cube.coord_system().as_cartopy_projection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c8973b-d130-46ff-b2d6-e910e86293c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert mask points to ndarray\n",
    "points = np.asarray(mask_points)\n",
    "# Transform them to the cube projection.\n",
    "transformed_points = target_projection.transform_points(\n",
    "    ccrs.PlateCarree(), points[0, :], points[1, :]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526167bf-f097-42f9-af44-89486cc16b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformed coordinates of the bounding box.\n",
    "np.save(\n",
    "    os.path.join(data_path, \"etc/region_points_transformed.npy\"),\n",
    "    transformed_points,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca601570-d546-4f42-935a-509d550be322",
   "metadata": {},
   "source": [
    "Create a constraint from the converted corner coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a896b80-6e9e-4e03-8ac1-c9243594dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the constraint.\n",
    "region_constraint = iris.Constraint(\n",
    "    grid_latitude=lambda v: transformed_points[:, 1].min()\n",
    "    < v\n",
    "    < transformed_points[:, 1].max(),\n",
    "    grid_longitude=lambda v: transformed_points[:, 0].min()\n",
    "    < v\n",
    "    < transformed_points[:, 0].max(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1990529-7ecb-4f95-8f9b-85f0b75925a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And extract the region.\n",
    "reg_cube = cube.extract(region_constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2835f-8786-4e7f-8f14-4f438d191f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7285e-11de-4eb8-b6bb-0d9c02c2ea3f",
   "metadata": {},
   "source": [
    "Look at the selected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2e81a4-8c41-40f9-bca0-aab641fd3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gävle point\n",
    "lat = 60.73284099330242\n",
    "lon = 17.09885344649177\n",
    "fig, ax = plt.subplots(figsize=(7, 9), subplot_kw={\"projection\": sweref})\n",
    "iplt.contourf(reg_cube[0, :, :], 30, axes=ax)\n",
    "ax.scatter([lon], [lat], s=50, transform=ccrs.PlateCarree(), label=\"Gävle\")\n",
    "ax.scatter(\n",
    "    mask_points[0],\n",
    "    mask_points[1],\n",
    "    s=20,\n",
    "    c=\"k\",\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    label=\"Box corners\",\n",
    ")\n",
    "\n",
    "ax.coastlines()\n",
    "ax.legend()\n",
    "# Set the extent to put the data into context.\n",
    "ax.set_extent([10, 20, 50, 75], crs=ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2caa584-d1f8-4656-8884-87ab8f61fe80",
   "metadata": {},
   "source": [
    "## Rx1 annual\n",
    "### Event definition\n",
    "It rained 161 mm in 24 hours in Gävle during the event.\n",
    "This gives us a threshold for the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51f300-34ed-4630-b89e-de4f064f5ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [mm s or kg/m2/s]\n",
    "threshold = 161"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f9a59-6100-40a8-b679-cdee738d9ac9",
   "metadata": {},
   "source": [
    "This however raises the question, is it a fair comparison to take the daily intensity of the gridded product and compare it to station data like this?\n",
    "\n",
    "We will use Climix to calculate the index we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f842537c-d235-4ae7-a52a-674fbd04e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index catalog\n",
    "catalog = load_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad1a5d-f4b6-4aa8-a21f-a9ae7ee6f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx1_ann_index = catalog.prepare_indices([\"rx1day\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500abf9-aa41-4a05-b0de-8fe7f21c3a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't have a year coordiante when passing to climix.\n",
    "try:\n",
    "    reg_cube.remove_coord(\"year\")\n",
    "except iris.exceptions.CoordinateNotFoundError:\n",
    "    pass\n",
    "rx1_ann = rx1_ann_index([reg_cube], client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c9ab5-60f6-4f1f-b204-6983ba04d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We mask the data with sweden.\n",
    "mask = iris_utils.mask_from_shape(\n",
    "    rx1_ann, swe_mainland, coord_names=[\"grid_latitude\", \"grid_longitude\"]\n",
    ")\n",
    "iris_utils.mask_cube(rx1_ann, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3338d911-9a50-42a0-b3ab-d7c51b29fb52",
   "metadata": {},
   "source": [
    "### Fitting an extreme value distribution to Rx1\n",
    "Now we can start looking at the extremes, e.g. annual Rx1.\n",
    "In this case Rx1 should simply be the annual max?\n",
    "Since we already have daily values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8f62b5-679a-4818-9f2e-9e581de4d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx1_ann.data.compressed().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eb7ce2-f1df-49e3-9c6d-f00f789f0c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, density is way above one since the bin values are so small.\n",
    "# e.g. the widht of each bin is ~0.0001, hence integrating to 1\n",
    "plt.hist(rx1_ann.data.compressed());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5fe056-cedd-46f6-bc2c-68b25755f355",
   "metadata": {},
   "source": [
    "We try and fit a number of extreme value distributions to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79574483-57fd-4419-94f6-acab9fdf4bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some distributions describing extremes.\n",
    "dists = {\n",
    "    \"genextreme\": scstats.genextreme,\n",
    "    \"genpareto\": scstats.genpareto,\n",
    "    \"gamma\": scstats.gamma,\n",
    "    \"gengamma\": scstats.gengamma,\n",
    "    \"gumbel_l\": scstats.gumbel_l,\n",
    "    \"gumbel_r\": scstats.gumbel_r,\n",
    "}\n",
    "# data\n",
    "data = rx1_ann.data.compressed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13083cc4-4074-4787-80c2-17c6c36ad388",
   "metadata": {},
   "source": [
    "Before we do the bootstrap, we want to check the goodness of fit for the distribution and the data.\n",
    "For this we use a Kolmogorov-Smirnof test (KS-test).\n",
    "For a goodness of fit this is a bit unintuitive.\n",
    "The 0-hypothesis is that the distributions are the same, hence we are looking for a high p-value here. e.g. that we can't say that the dists are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f4c2f-18aa-42ee-86f6-2ccdc139ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit each distribution and evaluate KS test.\n",
    "for key, dist in dists.items():\n",
    "    fit = dist.fit(data)\n",
    "    print(f\"{key}:\", scstats.ks_1samp(data, dist.cdf, args=fit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65d4fd-0eaa-42fb-acc7-1576c10153de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, density is way above one since the bin values are so small.\n",
    "x = np.linspace(0, 120, 200)\n",
    "# e.g. the widht of each bin is ~0.0001, hence integrating to 1\n",
    "plt.hist(rx1_ann.data.compressed(), bins=20, density=True)\n",
    "for key, dist in dists.items():\n",
    "    fit = dists[key].fit(data)\n",
    "    plt.plot(x, dists[key].pdf(x, *fit), label=key)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31151d-3a12-42cc-8a18-38fbc7c44217",
   "metadata": {},
   "source": [
    "For a KS-test high p-value = we can't reject the null hypothesis that they are from the same distributions.\n",
    "\n",
    "$\\rightarrow$ the GEV distribution has the better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c14a5-0f69-4349-9f06-65cf617a026f",
   "metadata": {},
   "source": [
    "### Regression to GMST\n",
    "To scale the above distribution with the use of GMST we first need to fit a regression between the Rx1 and GMST.\n",
    "The slope of the regression can then be used for the scaling.\n",
    "\n",
    "But first we load the GISTEMP data from NASA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f69cc-f678-4a8b-8eef-a56724c7451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmst_path = os.path.join(data_path, \"etc/gistemp.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a625214-9976-464d-b10f-e208bfe28e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the smoothed gmst data  for the timespan\n",
    "# covered by the cube.\n",
    "gmst_data = attribution.funcs.get_gmst(rx1_ann, path=gmst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5653b-7d71-4e42-ad16-57a080d500c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the data of the rx1 cube.\n",
    "rx1_ann_data = np.zeros((rx1_ann.shape[0], rx1_ann.data[0, :, :].compressed().shape[0]))\n",
    "# We need to compress the data for each year. This has to be done\n",
    "# in a loop I think.\n",
    "for i, year in enumerate(rx1_ann.data):\n",
    "    rx1_ann_data[i] = year.compressed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d8d5ad-cf6f-4be6-a65b-42fed659106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that first dimensions match.\n",
    "assert rx1_ann_data.shape[0] == gmst_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc09d2-0a7f-4433-9444-704955e90697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to look at the data.\n",
    "# fig, ax = plt.subplots(figsize=(7, 7))\n",
    "# ax.scatter(np.broadcast_to(gmst_data, rx1_ann_data.shape).flatten(),\n",
    "#                             rx1_ann_data.flatten(), s=5);\n",
    "# ax.set_xlabel(\"GMST\")\n",
    "# ax.set_ylabel(\"Precipitation intensity\");\n",
    "# ax.set_title(\"Pooled region scatter\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a641f73-da89-4073-bbe9-09d51bb8eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the linear regression we use Sklearn.\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86099adc-f3b1-47c6-acd7-5e77e28bba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can make clever use of the multiregression feature, we want\n",
    "# know the regression for each point.\n",
    "reg = LinearRegression().fit(gmst_data, rx1_ann_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab81e33-9914-45ea-af2f-5faf4e519044",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc07aa-7752-4e60-b735-2ec9880b1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx1_ann_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f24f34-e653-4494-8b5f-87f6266e4a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We broadcast the slopes to have a slope for each entry in the pooled data.\n",
    "slopes_broad = np.broadcast_to(reg.coef_.reshape(1, -1), rx1_ann_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791fcaf1-e5de-422e-a8ef-cfe931aeaf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_broad = slopes_broad.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b1190-6a28-48e2-8aa8-07a5fa04cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should now have the same shape.\n",
    "assert slopes_broad.shape == data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf668f4-80d4-4410-9e2d-d028191a11f9",
   "metadata": {},
   "source": [
    "We scale the distribution by making the location and scale a function of the temperature anomaly, using the slope of the regression.\n",
    "\n",
    "$\\mu = \\mu_0 \\mathrm{exp}(\\alpha T' / \\mu_0),\\, \\sigma = \\sigma_0\\mathrm{exp}(\\alpha T'/ \\mu_0)$\n",
    "\n",
    "This is implemented in the `attribution.scale_dist_params` which is used by `calc_prob_ratio` available in `attribution.funcs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef5cce3-3336-40ad-b012-ffd42fc4ef38",
   "metadata": {},
   "source": [
    "### Probabilities\n",
    "The bootstrap takes the variation in regression slope into account.\n",
    "For the resampling we are randomly selecting the regression coefficient for each resample.\n",
    "The jackknife works by leaving out the slope corresponding to the sample (day) that is left out.\n",
    "\n",
    "Randomly picking slopes should result in a good representation since the regression coefficients are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eaf941-82d7-4534-b359-3c765a8b3231",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(reg.coef_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a76a3a7-c013-4ba9-8b93-9ca7b4f25ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(attribution.funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcf070-a703-429a-acef-1f6055a66b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a random number generator.\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2866c13-8502-4641-8bdb-ae4dda66c319",
   "metadata": {},
   "source": [
    "Create some test data.\n",
    "Only used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3647f4db-6527-428e-b2de-c73b1a4fffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = rng.integers(0, data.shape[0], 7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0321ce-ecd1-4c5f-bc3b-5f10f48da07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data[..., test_indices]\n",
    "test_slopes = slopes_broad[..., test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b6739-84c8-4eab-9b7e-76c979696f4e",
   "metadata": {},
   "source": [
    "Create a partial function which we can pass to the bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1e3eb-e82b-42c1-ba50-ff041e43c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a partial function of calc_prob_ratio which can be passed\n",
    "# to the bootstrap.\n",
    "# temperature indicates to which temperature we scale the counterfactua\n",
    "# climate. In this case we want a climate that is 1.2 degrees colder.\n",
    "calc_prob_ratio_p = partial(\n",
    "    attribution.funcs.calc_prob_ratio,\n",
    "    threshold=threshold,\n",
    "    temperature=-1.2,\n",
    "    dist=dists[\"genextreme\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881af50-ac02-4318-8d64-72b070b9956d",
   "metadata": {},
   "source": [
    "First we calculate the probability ratio of the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba4e7fc-bb64-4def-ab8e-508f43113de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The probability ratio of the sample.\n",
    "rx1_ann_pbr = calc_prob_ratio_p(test_data, test_slopes)\n",
    "\n",
    "rx1_ann_pbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1830cb-fc62-42a9-854e-d36489a0145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(attribution.bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb85dc0-fed4-458a-aed9-7d084413d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the bootstrapped CI of the probability ratio\n",
    "# Note that this is much faster on an mp pool. \n",
    "rx1_ann_pbr_ci, rx1_ann_pbr_med, theta_hat_b = attribution.bootstrap.bootstrap_mp(\n",
    "    (data, slopes_broad), calc_prob_ratio_p, n_resamples=9999, batch=1, client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98dc683-ff83-42ab-851e-51cba1906793",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(theta_hat_b);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca5a2e-0fd6-4b21-b7f0-624e60ffbb64",
   "metadata": {},
   "source": [
    "The probability ratio(s) (PR) for an event the magnitude of the Gävle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589349f-b6b7-4424-beb4-49f146e09c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx1_ann_pbr_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd332e-c7fa-41f2-b8cf-47052bedf248",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_ratios_ci = np.asarray(\n",
    "    [\n",
    "        rx1_ann_pbr_ci.confidence_interval.low,\n",
    "        rx1_ann_pbr_med,\n",
    "        rx1_ann_pbr_ci.confidence_interval.high,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf1ec3-4adf-4dc8-be20-c47eb15647c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_ratios_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f80fe3-c803-4b73-a6f7-f39a85ece813",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(data_path, \"etc/rx1-ann_pbr_gridclim\"), prob_ratios_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c166e9a-4524-4865-97ca-f188e1f023a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.load(os.path.join(data_path, \"etc/rx1-ann_pbr_gridclim.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e128e858-a449-4c25-bffd-aeebd7cf34ff",
   "metadata": {},
   "source": [
    "Since the PR CI include 1 we cannot make a attribution statement for this event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a307365-17c5-4dc0-938a-6111b501b0e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Rx1 seasonal\n",
    "### Event definition\n",
    "- It rained 161 mm in 24 hours in Gävle during the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d79cf0-e4fc-46fc-bc88-fa80321db024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [mm s or kg/m2/s]\n",
    "threshold = 161"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c6af8-389d-4569-bc2d-e805e82ac1a0",
   "metadata": {},
   "source": [
    "which can define as the event to look for.\n",
    "\n",
    "We can do this quickly in the whole of GridClim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ffcec-6a4d-4ad4-a480-d74995c19e45",
   "metadata": {},
   "source": [
    "This however raises the question, is it a fair comparison to take the daily intensity of the gridded product and compare it to station data like this?\n",
    "\n",
    "We will use Climix to calculate the index we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64084ed9-b275-4ee2-8986-411c447e152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index catalog\n",
    "# Make sure that default is changed in the yml file.\n",
    "catalog = load_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae1edb-249b-408f-a42e-f8b4893e4660",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx1_seasonal_index = catalog.prepare_indices([\"rx1day\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d98b21-82aa-4f4c-88e1-f8622d21c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't have a year coordiante when passing to climix.\n",
    "try:\n",
    "    reg_cube.remove_coord(\"year\")\n",
    "except iris.exceptions.CoordinateNotFoundError:\n",
    "    pass\n",
    "try:\n",
    "    reg_cube.remove_coord(\"season_year\")\n",
    "except iris.exceptions.CoordinateNotFoundError:\n",
    "    pass\n",
    "try:\n",
    "    reg_cube.remove_coord(\"season\")\n",
    "except iris.exceptions.CoordinateNotFoundError:\n",
    "    pass\n",
    "rx1_seasonal = rx1_seasonal_index([reg_cube], client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3a80f-2f8a-48ee-b5ae-22876a33a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We mask the data with sweden.\n",
    "mask = iris_utils.mask_from_shape(\n",
    "    rx1_seasonal, swe_mainland, coord_names=[\"grid_latitude\", \"grid_longitude\"]\n",
    ")\n",
    "iris_utils.mask_cube(rx1_seasonal, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f56687-240f-4cc5-b3d3-7e45f1fab0c9",
   "metadata": {},
   "source": [
    "We select only JJA for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ea613-7e18-4b0d-aba1-110ae6e05304",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = iris.Constraint(season=\"jja\")\n",
    "rx1_jja = rx1_seasonal.extract(constraint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682c2af7-6dfc-4167-90e6-0921c91c760e",
   "metadata": {},
   "source": [
    "### Fitting an extreme value distribution to Rx1\n",
    "Now we can start looking at the extremes, e.g. annual Rx1.\n",
    "In this case Rx1 should simply be the annual max?\n",
    "Since we already have daily values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fcb855-3a86-454e-8e5f-263039724c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, density is way above one since the bin values are so small.\n",
    "# e.g. the widht of each bin is ~0.0001, hence integrating to 1\n",
    "plt.hist(rx1_jja.data.compressed());\n",
    "plt.hist(rx1_ann.data.compressed(), alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74013a37-7053-4966-a949-15d3c9442799",
   "metadata": {},
   "source": [
    "We try and fit a number of extreme value distributions to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99decb31-6a6b-4dae-9ee5-d8d795479d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the GEV dist object\n",
    "dists = {\n",
    "    \"genextreme\": scstats.genextreme,\n",
    "    \"genpareto\": scstats.genpareto,\n",
    "    \"gamma\": scstats.gamma,\n",
    "    \"gengamma\": scstats.gengamma,\n",
    "    \"gumbel_l\": scstats.gumbel_l,\n",
    "    \"gumbel_r\": scstats.gumbel_r,\n",
    "}\n",
    "# data\n",
    "data = rx1_jja.data.compressed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba6545f-95f4-4518-8cc3-2672c22cf5ca",
   "metadata": {},
   "source": [
    "Before we do the bootstrap, we want to check the goodness of fit for the distribution and the data.\n",
    "For this we use a Kolmogorov-Smirnof test (KS-test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e5de8-a527-43fb-8e9d-92628519438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, dist in dists.items():\n",
    "    fit = dist.fit(data)\n",
    "    print(f\"{key}:\", scstats.ks_1samp(data, lambda x: dist.cdf(x, *fit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e039bac-b133-4eba-ba99-e580d0755fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, density is way above one since the bin values are so small.\n",
    "x = np.linspace(0, 120, 200)\n",
    "# e.g. the widht of each bin is ~0.0001, hence integrating to 1\n",
    "plt.hist(data, density=True)\n",
    "for key, dist in dists.items():\n",
    "    fit = dist.fit(data)\n",
    "    plt.plot(x, dist.pdf(x, *fit), label=key)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ebb768-2bca-4537-8580-891ff1f09510",
   "metadata": {},
   "source": [
    "For a KS-test high p-value = we can't reject the null hypothesis that they are from different distributions.\n",
    "Thus, the GEV has the best performance, even if it is not great."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd002e-ed08-49ef-82a1-15d7c36b35f0",
   "metadata": {},
   "source": [
    "### Regression to GMST\n",
    "To scale the above distribution with the use of GMST we first need to fit a regression between the Rx1 and GMST.\n",
    "The slope of the regression can then be used for the scaling.\n",
    "\n",
    "But first we load the GISTEMP data from NASA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163ba3c-76a4-46be-a2d8-8091c0a4635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmst_path = os.path.join(data_path, \"etc/gistemp.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cb5a1c-dd3d-4cf8-b795-39983c3f3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(attribution.funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c3775-ea42-42c4-ac91-ff57c9b70eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the smoothed gmst data  for the timespan\n",
    "# covered by the cube.\n",
    "gmst_data = attribution.funcs.get_gmst(rx1_jja, path=gmst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f0ae8-d156-4e5b-abe0-a041e30af325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the data of the rx1 cube.\n",
    "rx1_jja_data = np.zeros((rx1_jja.shape[0], rx1_jja.data[0, :, :].compressed().shape[0]))\n",
    "# We need to compress the data for each year. This has to be done\n",
    "# in a loop I think.\n",
    "for i, year in enumerate(rx1_jja.data):\n",
    "    rx1_jja_data[i] = year.compressed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3363cae-b377-4911-87f9-d62acdea1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that first dimensions match.\n",
    "assert rx1_jja_data.shape[0] == gmst_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ba1fb-a753-4057-a5ac-edfc9431e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the linear regression we use Sklearn.\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76280f-467d-43bf-a8da-699c3d4d1960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can make clever use of the multiregression feature, we want\n",
    "# know the regression for each point.\n",
    "reg = LinearRegression().fit(gmst_data, rx1_jja_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a1d45-f6f5-44ae-aabe-79756e7ddcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We broadcast the slopes to have a slope for each entry in the pooled data.\n",
    "slopes_broad = np.broadcast_to(reg.coef_.reshape(1, -1), rx1_jja_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2062b11-cc27-45f3-9936-20c5b7f500b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_broad = slopes_broad.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b62e8b-00d6-4c15-bca9-5fc4c006de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should now have the same shape.\n",
    "assert slopes_broad.shape == data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c234812-23c0-4458-b9f5-5e612ad10c7b",
   "metadata": {},
   "source": [
    "### Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42e60b6-ba30-4edb-8706-1eb375ac55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = rng.integers(0, data.shape[0], 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813e9ca-fb3b-4c1c-bd82-c14afef0ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data[..., test_indices]\n",
    "test_slopes = slopes_broad[..., test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed4dcb-19c9-4feb-96ed-a340018b7a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = dists[\"genextreme\"].fit(rx1_jja.data.compressed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901a317f-afc2-48be-9476-5069e6ff62b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - dists[\"genextreme\"].cdf(161, *fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b03ceb-1fbb-4432-a224-1ee99a149acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(attribution.funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7624376-cd85-4336-912d-4d12b5708b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a partial function of calc_prob_ratio which can be passed\n",
    "# to the bootstrap.\n",
    "# temperature indicates to which temperature we scale the counterfactua\n",
    "# climate. In this case we want a climate that is 1.2 degrees colder.\n",
    "calc_prob_ratio_p = partial(\n",
    "    attribution.funcs.calc_prob_ratio,\n",
    "    threshold=50,\n",
    "    temperature=-1.2,\n",
    "    dist=dists[\"genextreme\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec5548-2435-4bab-a94b-2b0e703fd127",
   "metadata": {},
   "source": [
    "Calculate the probability ratio CI, median and theta_hat_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97696de9-4f38-4731-aa26-6586797f8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(attribution.bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b79974d-cd25-4d38-8823-2b8946bc36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the bootstrapped CI of the probability ratio\n",
    "rx1_jja_pbr_ci, rx1_jja_pbr_med, theta_hat_b = attribution.bootstrap.bootstrap_mp(\n",
    "    (test_data, test_slopes), calc_prob_ratio_p, n_resamples=1000, batch=1, client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067b728-ae0c-4ddc-99c5-ebd0963f3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(theta_hat_b);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38da1dc-5ac6-4fe2-8c2e-8125b4e261be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx1_jja_pbr_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca48f32-4d47-422d-b2b0-a90c1e6f58e4",
   "metadata": {},
   "source": [
    "The probability ratio(s) (PR) for an event the magnitude of the Gävle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013353cd-6d2e-4b46-85d2-5aab2c0dd669",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_ratios_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9678fd-1ca5-4bb8-8d6b-7c3f9f52e847",
   "metadata": {},
   "source": [
    "The probability ratio(s) (PR) for an event the magnitude of the Gävle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55824aff-74f8-4482-9eea-ed57f314c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(data_path, \"etc/rx1-jja_prb_gridclim\"), prob_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc28a010-fd50-421f-8452-dfcc7000ee78",
   "metadata": {},
   "source": [
    "Since the PR CI include 1 we cannot make a attribution statement for this event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f918d-9d26-4463-b158-ca772b9ae479",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Rx2\n",
    "First we have to calculate the Rx2 index for the GridClim data.\n",
    "This can be done using the Climix package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84afc117-9cb1-414c-a8df-ff4112a512c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3cb003-f107-472c-92b6-d5e3c7371a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx2_ann_index = catalog.prepare_indices([\"rx2day\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e9962-0523-4aa8-baee-0f14c2c84036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cube can't have a year coordinate.\n",
    "try:\n",
    "    reg_cube.remove_coord(\"year\")\n",
    "except iris.exceptions.CoordinateNotFoundError:\n",
    "    pass\n",
    "# Calculate the index.\n",
    "rx2_ann = rx2_ann_index([reg_cube], client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3414b6-dcd2-4d08-873b-4e18c79091f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We mask the data with sweden.\n",
    "mask = iris_utils.mask_from_shape(\n",
    "    rx2_ann, swe_mainland, coord_names=[\"grid_latitude\", \"grid_longitude\"]\n",
    ")\n",
    "iris_utils.mask_cube(rx2_ann, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c4d56-71f6-4d7a-802c-22389f13f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iplt.contourf(rx2_ann[0, :, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb72e8-5cd7-4bdb-8fd4-52ac9f81527b",
   "metadata": {},
   "source": [
    "### Event definition\n",
    "The Gävle event was a single day event, thus a clear definition is not available.\n",
    "So what should we pick here? \n",
    "\n",
    "- Based on percentiles? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bcc103-19b0-4d32-999a-467c1602fc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01a3a307-12d6-480e-b048-19127e2c1c21",
   "metadata": {},
   "source": [
    "### Fitting an extreme event distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d834f2e6-01f3-4fc2-9468-a1db370f06a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx2_ann_data = rx2_ann.data.compressed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0341c41e-f367-44ff-8956-0fb7cc5c5551",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx2_ann_data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf860aa-33ac-4d6e-86f8-a15d368cec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rx2_ann_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b0fe8c-2ad8-446b-9382-e59a6ecb4419",
   "metadata": {},
   "source": [
    "We try and fit a number of extreme value distributions to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d9cac-5200-493b-8347-133042855fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the GEV dist object\n",
    "dists = {\n",
    "    \"genextreme\": scstats.genextreme,\n",
    "    \"genpareto\": scstats.genpareto,\n",
    "    \"gamma\": scstats.gamma,\n",
    "    \"gengamma\": scstats.gengamma,\n",
    "    \"gumbel_l\": scstats.gumbel_l,\n",
    "    \"gumbel_r\": scstats.gumbel_r,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1471844-364f-451f-abda-7daafa4e1b85",
   "metadata": {},
   "source": [
    "Before we do the bootstrap, we want to check the goodness of fit for the distribution and the data.\n",
    "For this we use a Kolmogorov-Smirnof test (KS-test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb70680c-3043-4719-bc37-fe9882ad4498",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, dist in dists.items():\n",
    "    fit = dist.fit(rx2_ann_data)\n",
    "    print(f\"{key}:\", scstats.ks_1samp(rx2_ann_data, lambda x: dist.cdf(x, *fit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0f2ea-722f-4c8e-81e2-ebf2498887e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, density is way above one since the bin values are so small.\n",
    "x = np.linspace(0, 120, 200)\n",
    "# e.g. the widht of each bin is ~0.0001, hence integrating to 1\n",
    "plt.hist(rx2_ann_data, bins=20, density=True)\n",
    "for key, dist in dists.items():\n",
    "    fit = dists[key].fit(rx2_ann_data)\n",
    "    plt.plot(x, dists[key].pdf(x, *fit), label=key)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a7332-ade7-4e8a-803f-f13b5f3155fb",
   "metadata": {},
   "source": [
    "For a KS-test high p-value = we can't reject the null hypothesis that they are from different distributions.\n",
    "Thus, the GEV has the best performance, even if it is not great."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeae542-9448-4707-b84d-83d403406812",
   "metadata": {},
   "source": [
    "### Regression to GMST\n",
    "To scale the above distribution with the use of GMST we first need to fit a regression between the Rx1 and GMST.\n",
    "The slope of the regression can then be used for the scaling.\n",
    "\n",
    "But first we load the GISTEMP data from NASA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1fed75-c55c-4cf8-8462-243c01650f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmst_path = os.path.join(data_path, \"etc/gistemp.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8563e6-1fb6-46fd-8ecb-1834ae238ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the smoothed gmst data  for the timespan\n",
    "# covered by the cube.\n",
    "gmst_data = attribution.funcs.get_gmst(rx2_ann, path=gmst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fbc88a-a61d-418f-9e4a-bffbb0df9fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the data of the rx1 cube.\n",
    "# Reshape to flatten the spatial dimensions.\n",
    "rx2_ann_data_reg = rx2_ann.data.reshape(rx2_ann.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d41736f-13fe-4998-b4b7-7ee3fabe145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that first dimensions match.\n",
    "assert rx2_ann_data_reg.shape[0] == gmst_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3be681-0f0f-40e0-8e00-0f27b96be2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to look at the data.\n",
    "# fig, ax = plt.subplots(figsize=(7, 7))\n",
    "# ax.scatter(np.broadcast_to(gmst_data, rx1_ann_data.shape).flatten(),\n",
    "#                             rx1_ann_data.flatten(), s=5);\n",
    "# ax.set_xlabel(\"GMST\")\n",
    "# ax.set_ylabel(\"Precipitation intensity\");\n",
    "# ax.set_title(\"Pooled region scatter\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737aa5b5-e95e-491a-9c80-bfcbec31bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the linear regression we use Sklearn.\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2ffc1-7182-4edb-826a-c3aad4927d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can make clever use of the multiregression feature, we want\n",
    "# know the regression for each point.\n",
    "reg = LinearRegression().fit(gmst_data, rx2_ann_data_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6673edfe-62ac-4fbb-adc9-1f33059b2f29",
   "metadata": {},
   "source": [
    "We scale the distribution by making the location and scale a function of the temperature anomaly, using the slope of the regression.\n",
    "\n",
    "$\\mu = \\mu_0 \\mathrm{exp}(\\alpha T' / \\mu_0),\\, \\sigma = \\sigma_0\\mathrm{exp}(\\alpha T'/ \\mu_0)$\n",
    "\n",
    "This is implemented in the `attribution.scale_dist_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e422b5e-ea61-420e-bd0a-7a4c14f1dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create current climate dists with CI\n",
    "dists_ci = [dists[\"genextreme\"](*fit) for fit in fits_ci]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783fd9de-f2b7-463f-8d31-d0e953e3d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scaled_dists = attribution.funcs.scale_distributions(\n",
    "    fits_ci, reg, dists[\"genextreme\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585d800-0881-4f46-b4b6-bd8fb4c11d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribution.plotting.plot_distribution(\n",
    "    data, dists_ci, all_scaled_dists, title=\"Rx2 GridClim\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ed0ac5-7cb5-4b08-85ca-5f94e9eb68a9",
   "metadata": {},
   "source": [
    "### Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc06250e-7231-494b-bfaa-842e98174a7b",
   "metadata": {},
   "source": [
    "The probability ratio(s) (PR) for an event the magnitude of the Gävle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420491e4-de35-4373-a9b5-713797046b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_ratios = attribution.funcs.get_probability_ratios(\n",
    "    dists_ci, all_scaled_dists, threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c4fae-6e99-469f-a127-0d098af5009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(data_path, \"etc/rx2-ann_prb_gridclim\"), prob_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912c62e-c034-426a-a6f9-a2239859e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3677e476-e910-4ce7-8700-836e1dc536a7",
   "metadata": {},
   "source": [
    "Since the PR CI include 1 we cannot make a attribution statement for this event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d109fe-792b-431c-bc81-81ee16e2cbb5",
   "metadata": {},
   "source": [
    "## Next step\n",
    "\n",
    "[Analysing EOBS](./eobs.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
